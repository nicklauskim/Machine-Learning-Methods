{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## The Iterated Reweighted Least Squares Algorithm in Python"
      ],
      "metadata": {
        "id": "yLB1y6cac4om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import linalg"
      ],
      "metadata": {
        "id": "mCFfaOW2dIOK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a default seed to make debugging easier\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "NL4hOhpnjiyp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_logistic(x, y, epsilon = 1e-6):\n",
        "  '''\n",
        "  Input:\n",
        "    x: An n x p matrix where each row represents a single data point of dimension p\n",
        "    y: An n x 1 vector of labels: True or False, where each entry corresponds to each row in x\n",
        "    epsilon: set by default to 1e-6\n",
        "  Output:\n",
        "    beta: An n x 1 vector of coefficients optimizing the logistic regression problem    (p x 1)??\n",
        "    losses: An array of losses keeping record of the progression in the optimization\n",
        "  '''\n",
        "\n",
        "  # Initialize a list to keep record of all the losses\n",
        "  losses = []\n",
        "\n",
        "  # Set number of rows, columns\n",
        "  r, c = x.shape\n",
        "\n",
        "  # Initialize our beta vector to be all ones    ##############\n",
        "  beta = np.ones((c, 1))\n",
        "\n",
        "  # Run while loop until we are within an epsilon error\n",
        "  err = 1\n",
        "  while err > epsilon:\n",
        "    # Matrix x times vector beta, to produce a vector of signals (scores)\n",
        "    s = np.dot(x, beta)\n",
        "\n",
        "    # Compute probability\n",
        "    pr = sigmoid(s)\n",
        "\n",
        "    # In class, we showed that l''(s_i) = -p_i(1 - p_i) := -w_i\n",
        "    # So, we compute the weights based on the probability computed from the signal s\n",
        "    w = pr * (1 - pr)\n",
        "\n",
        "    # Again from class, this is equivalent to e_i / w_i\n",
        "    y_hat = (y - pr) / w\n",
        "\n",
        "    # Adjust parameters for partial least-squares solution - multiplying by the square root of weights (an n x 1 vector)\n",
        "    sw = np.sqrt(w)\n",
        "    mw = np.repeat(sw, c, axis = 1)\n",
        "    x_work = mw * x\n",
        "    y_work = sw * y_hat\n",
        "\n",
        "    # Solve partial least-squares problem\n",
        "    delta_beta, _, _, _ = np.linalg.lstsq(x_work, y_work)\n",
        "\n",
        "    # Recompute the error\n",
        "    err = np.sum(np.abs(delta_beta))\n",
        "\n",
        "    # Keep record of the error\n",
        "    losses.append(err)\n",
        "\n",
        "    # Adjust beta\n",
        "    beta = beta + delta_beta\n",
        "\n",
        "  return beta, np.array(losses)"
      ],
      "metadata": {
        "id": "3p03hLjFji3z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "0UX7miA2cDym"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 100\n",
        "p = 2    # just to keep it easy to visualize\n",
        "X = np.random.normal(0, 1, (n, p))\n",
        "print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-N3UHkxbL0D",
        "outputId": "715c9c48-417d-4561-9200-35660a8965cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beta = np.ones((p, 1))\n",
        "print(\"beta: \", beta)\n",
        "print(\"beta shape: \", beta.shape)\n",
        "print(\"----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IJ-SXlLcbgJ",
        "outputId": "ad96ab4b-a3ec-4007-f864-ecd80e172085"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta:  [[1.]\n",
            " [1.]]\n",
            "beta shape:  (2, 1)\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.random.uniform(0, 1, (n, 1)) < sigmoid(np.dot(X, beta)).reshape((n, 1))"
      ],
      "metadata": {
        "id": "aTAjp6O5chHo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta_hat, losses = my_logistic(X, Y)\n",
        "print(\"beta_hat: \", beta_hat)\n",
        "print(\"beta_hat shape: \", beta_hat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQxWmzx2d02E",
        "outputId": "d52a09d6-eb7b-487b-be02-1a98f69dc533"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beta_hat:  [[0.46594939]\n",
            " [1.20195238]]\n",
            "beta_hat shape:  (2, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-b70fb2a9a8c0>:44: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "  delta_beta, _, _, _ = np.linalg.lstsq(x_work, y_work)\n"
          ]
        }
      ]
    }
  ]
}