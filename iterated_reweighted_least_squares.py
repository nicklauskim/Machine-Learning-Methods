# -*- coding: utf-8 -*-
"""homework1-code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n_xdJYOMTCkUkRUCF9CV5iNQHpisQuvf

## The Iterated Reweighted Least Squares Algorithm in Python
"""

import numpy as np
from scipy import linalg

# Set a default seed to make debugging easier
np.random.seed(0)

def my_logistic(x, y, epsilon = 1e-6):
  '''
  Input:
    x: An n x p matrix where each row represents a single data point of dimension p
    y: An n x 1 vector of labels: True or False, where each entry corresponds to each row in x
    epsilon: set by default to 1e-6
  Output:
    beta: An n x 1 vector of coefficients optimizing the logistic regression problem    (p x 1)??
    losses: An array of losses keeping record of the progression in the optimization
  '''

  # Initialize a list to keep record of all the losses
  losses = []

  # Set number of rows, columns
  r, c = x.shape

  # Initialize our beta vector to be all ones    ##############
  beta = np.ones((c, 1))

  # Run while loop until we are within an epsilon error
  err = 1
  while err > epsilon:
    # Matrix x times vector beta, to produce a vector of signals (scores)
    s = np.dot(x, beta)

    # Compute probability
    pr = sigmoid(s)

    # In class, we showed that l''(s_i) = -p_i(1 - p_i) := -w_i
    # So, we compute the weights based on the probability computed from the signal s
    w = pr * (1 - pr)

    # Again from class, this is equivalent to e_i / w_i
    y_hat = (y - pr) / w

    # Adjust parameters for partial least-squares solution - multiplying by the square root of weights (an n x 1 vector)
    sw = np.sqrt(w)
    mw = np.repeat(sw, c, axis = 1)
    x_work = mw * x
    y_work = sw * y_hat

    # Solve partial least-squares problem
    delta_beta, _, _, _ = np.linalg.lstsq(x_work, y_work)

    # Recompute the error
    err = np.sum(np.abs(delta_beta))

    # Keep record of the error
    losses.append(err)

    # Adjust beta
    beta = beta + delta_beta

  return beta, np.array(losses)

def sigmoid(x):
  return 1 / (1 + np.exp(-x))

n = 100
p = 2    # just to keep it easy to visualize
X = np.random.normal(0, 1, (n, p))
print("----")

beta = np.ones((p, 1))
print("beta: ", beta)
print("beta shape: ", beta.shape)
print("----")

Y = np.random.uniform(0, 1, (n, 1)) < sigmoid(np.dot(X, beta)).reshape((n, 1))

beta_hat, losses = my_logistic(X, Y)
print("beta_hat: ", beta_hat)
print("beta_hat shape: ", beta_hat.shape)